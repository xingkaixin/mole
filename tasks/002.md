# 数据探查工具

## 技术栈
一个wails的程序，界面是用react写的，ui是用shadcn@ui的组件库写的。

## 基本的功能
访问数据库，访问指定的表。
对表做一个数据分析，例如数据总量，业务唯一性检查，字段空值率检查，然后一个结果报告。
需要考虑会有一些非常大的表的场景，需要很好的去处理。
访问数据库的连接信息和访问的表都是页面配置数据库连接信息和勾选要分析的表。
执行时需要有进度条，整个任务的进度条，每张单表的进度条。

## 详细的需求展开
- 数据库连接信息（类型、地址、端口、用户名、密码、数据库名）。
- 连接性检查，通过后获取表清单
- 待分析的表列表。
- 勾选要分析的表，添加进任务队列，支持多选
- 任务队列的表，可以配置每张表需要做的任务
    - 数据总量
    - 业务唯一性检查（单字段、多字段组合）
    - 字段空值率检查
    - 其他你觉得有必要的检查（待定）

## 注意点
### 数据库连接
    * 程序需要根据配置信息动态连接到指定的数据库（如 MySQL, PostgreSQL 等）。
    * 需要妥善处理连接失败、认证失败等异常情况。

### 数据分析任务
    * 数据总量 (Total Rows): 对指定的每张表，执行 COUNT(*) 或类似操作获取总行数。这通常是最高效的方式。
    * 业务唯一性检查 (Uniqueness Check):
        * 对配置中指定的单字段或多字段组合进行唯一性校验。
        * 例如，检查 users 表的 email 字段是否存在重复值。
        * 需要报告重复的值以及它们的出现次数，方便排查。
    * 字段空值率检查 (Null Value Rate Check):
        * 对配置中指定的字段，计算其 NULL 或空字符串值的占比。
        * 计算公式为：(空值数量 / 数据总量) * 100%。


### 进度展示 (Progress Indication)
    * 实现一个清晰的进度条界面。
    * **整体进度条：**显示总共有多少张表需要处理，以及当前完成了几张。
    * **单表进度条：**在处理某张大表时（特别是进行唯一性或空值率检查时），显示该表的数据处理进度。

### 结果报告 (Result Reporting)
    * 所有分析任务完成后，在命令行标准输出中生成一份结构清晰的报告。
    * 报告应包含每张表的分析结果，例如：
        * 表名：orders
        * 数据总量：15,023,456
        * 唯一性检查 (order_id): 通过
        * 唯一性检查 (payment_id): 失败 (发现 12 个重复值)
        * 空值率检查 (shipping_address): 3.25%


### 非功能性需求 (Non-functional Requirements)
1. 性能与资源控制 (Performance & Resource Control)
    * **核心挑战：**处理超大表时不能将数据一次性全部加载到内存中，否则会导致内存溢出（OOM）。
    * 必须采用流式处理（Streaming）的方式从数据库读取数据。
2. 并发处理 (Concurrency)
    * 如果配置了多张表，程序应该能够并行处理这些表，以最大化利用 CPU 和 I/O 资源，缩短总执行时间。
3. 健壮性与错误处理 (Robustness & Error Handling)
    * 当某张表或某个分析任务失败时（如表不存在、字段不存在、数据库断开连接），程序不应直接崩溃。
    * 应记录错误信息，并可选择跳过失败的任务继续执行其他任务。
4. 易用性 (Usability)
    * 输出的报告和进度条应易于理解。

## 技术方案建议
* 数据库交互 (Database Interaction):
    * 使用 Go 标准库 database/sql 提供的接口。
    * 根据配置文件中的数据库类型，动态加载对应的驱动，例如 github.com/go-sql-driver/mysql 用于 MySQL，github.com/lib/pq 用于 PostgreSQL。
* 大表处理策略 (Handling Large Tables):
    * 关键： 绝对不要使用 SELECT * FROM a_large_table 然后一次性读取所有结果。
    * 正确做法： 使用 db.Query() 返回一个 *sql.Rows 对象。然后通过循环调用 rows.Next() 来逐行读取数据。这是一种流式处理，每次只在内存中保留少量数据。
    * 对于 数据总量，直接执行 SELECT COUNT(*) FROM table，数据库本身对这个操作有优化，速度很快，且不消耗应用内存。
    * 对于 唯一性检查 和 空值率检查，通过 SELECT column1, column2 FROM a_large_table 流式获取数据。
        * 唯一性： 在内存中使用一个 map[interface{}]int 来记录每个值出现的次数。如果 map 变得过大（极端情况），需要考虑更复杂的外部排序或基于磁盘的算法，但这在大多数场景下非必要。
        * 空值率： 在流式读取时，维护一个 null_count 和 total_count 的计数器即可，内存占用极小。


## 需要澄清的问题与潜在风险 (Clarifications & Potential Issues)
在开始开发前，我建议与你澄清以下几点，这会极大影响设计和实现复杂度：
1. 业务唯一性的具体定义？
    * 是单字段唯一（如 email），还是多字段组合唯一（如 (first_name, last_name, birth_date)）？
    * 如果需要支持多字段组合，配置文件的结构和唯一性检查的逻辑会更复杂。
2. 结果报告的格式要求？
    * 页面上展示，需要有导出的功能
3. 处理超超大规模表（例如：百亿级）的唯一性检查？
    * 对于几十亿、上百亿行的表，即便使用流式读取，在内存中维护一个 map 来检查唯一性也可能会耗尽内存。
    * 这是一个潜在的“不合理”点：对没有数据库索引的巨大表做唯一性检查本身就是一个极其耗时和耗资源的操作。如果存在这种需求，我们可能需要讨论：
        * 方案A (性能优先): 要求被检查的字段必须有数据库索引。这样可以通过 GROUP BY 和 HAVING COUNT(*) > 1 的 SQL 查询高效完成，但对数据库有要求。
        * 方案B (通用性优先): 如果不能依赖索引，需要接受分析过程会非常缓慢，并可能消耗大量内存。我们可以设置内存使用阈值，或采用更高级的概率性数据结构（如 HyperLogLog）来估算，但这会牺牲准确性。
4. 错误处理策略？
    * 当分析某张表失败时，是应该立即停止整个程序，还是记录错误后继续分析下一张表？推荐后者，并在最终报告中汇总所有错误。
5. 支持哪些数据库类型？
    * 先支持Mysql，代码要设计能扩展，未来支持更多的数据库。